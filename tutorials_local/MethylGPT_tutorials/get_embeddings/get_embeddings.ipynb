{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e253d29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T19:19:55.931080Z",
     "iopub.status.busy": "2025-01-22T19:19:55.930300Z",
     "iopub.status.idle": "2025-01-22T19:20:07.788513Z",
     "shell.execute_reply": "2025-01-22T19:20:07.787538Z"
    },
    "papermill": {
     "duration": 11.864861,
     "end_time": "2025-01-22T19:20:07.790224",
     "exception": false,
     "start_time": "2025-01-22T19:19:55.925363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/umap/__init__.py:9: ImportWarning: Tensorflow not installed; ParametricUMAP will be unavailable\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
      "  self.hub = sentry_sdk.Hub(client)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import argparse\n",
    "import ast\n",
    "import copy\n",
    "import gc\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter, OrderedDict\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from methylgpt.model.methyl_datasets import create_dataloader\n",
    "from methylgpt.model.methyl_model import MethylGPTModel\n",
    "from methylgpt.model.methyl_vocab import MethylVocab\n",
    "from methylgpt.model.methyl_loss import masked_mse_loss\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch\n",
    "\n",
    "from methylgpt.utils.plot_embeddings import plot_umap_categorical, plot_umap_numerical\n",
    "from methylgpt.utils.logging import *\n",
    "from methylgpt.common_setup import *\n",
    "\n",
    "try:\n",
    "    from flash_attn.flash_attention import FlashMHA\n",
    "\n",
    "    flash_attn_available = True\n",
    "except ImportError:\n",
    "    import warnings\n",
    "\n",
    "    warnings.warn(\"flash_attn is not installed\")\n",
    "    flash_attn_available = False\n",
    "\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf2e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T19:20:07.798813Z",
     "iopub.status.busy": "2025-01-22T19:20:07.797933Z",
     "iopub.status.idle": "2025-01-22T19:20:07.812618Z",
     "shell.execute_reply": "2025-01-22T19:20:07.811812Z"
    },
    "papermill": {
     "duration": 0.020278,
     "end_time": "2025-01-22T19:20:07.814133",
     "exception": false,
     "start_time": "2025-01-22T19:20:07.793855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to Embeddings\n",
      "{'seed': 42, 'input_type': 'CpGs_type3', 'parquet_dir': '../data/pretraining/processed_type3_parquet_shuffled', 'probe_id_dir': '../data/pretraining/probe_ids_type3.csv', 'qced_data_table': '../data/pretraining/QCed_samples_type3.csv', 'compiled_data_dir': '/home/A.Y/project/MethylGPT_clean/data/pretraining/compiled_metadata.csv', 'valid_ratio': 0.1, 'n_hvg': 49156, 'max_fi': 500000, 'do_train': True, 'pretrained_file': None, 'mask_ratio': 0.3, 'GEPC': True, 'dab_weight': 1.0, 'pretraining_dataset_name': 'CpGs_type3', 'epochs': 100, 'ecs_thres': 0.0, 'lr': 0.001, 'batch_size': 32, 'layer_size': 64, 'nlayers': 6, 'nhead': 4, 'dropout': 0.1, 'schedule_ratio': 0.9, 'save_eval_interval': 10, 'log_interval': 1000, 'fast_transformer': True, 'pre_norm': False, 'amp': True, 'pad_token': '<pad>', 'special_tokens': ['<pad>', '<cls>', '<eoc>'], 'mask_value': -1, 'pad_value': -2, 'explicit_zero_prob': False, 'max_seq_len': 49157, 'per_seq_batch_sample': False}\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = Path('Embeddings')\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {SAVE_DIR}\")\n",
    "\n",
    "PARQUET_DIR=\"/home/A.Y/project/MethylGPT_clean/data/pretraining/processed_type3_parquet_shuffled\"\n",
    "MODEL_PATH_DIR=\"/home/A.Y/project/MethylGPT_clean/pretrained_models/dev_pretraining_test-dataset_CpGs_type3-preprocessing_False-Sep26-10-27\"\n",
    "MODEL_DIR=MODEL_PATH_DIR+\"/model_epoch10.pt\"\n",
    "CPG_LIST_DIR=\"/home/A.Y/project/MethylGPT_clean/data/pretraining/probe_ids_type3.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# load from config file\n",
    "with open(Path(MODEL_PATH_DIR+\"/args.json\"), \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(config)\n",
    "\n",
    "# update config dict\n",
    "config[\"load_model\"] = True\n",
    "config[\"batch_size\"] = 32\n",
    "config[\"model_file\"] = MODEL_DIR\n",
    "config[\"mask_ratio\"] = 0\n",
    "config[\"probe_id_dir\"] = CPG_LIST_DIR\n",
    "\n",
    "\n",
    "\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "\n",
    "mask_ratio = config[\"mask_ratio\"]\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "\n",
    "# number of highly variable CpG sites\n",
    "n_hvg = config[\"n_hvg\"]  \n",
    "max_seq_len = n_hvg + 1\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "DSBN = True  # Domain-spec batchnorm\n",
    "explicit_zero_prob = False  # whether explicit bernoulli for zeros\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355836ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T19:20:07.821473Z",
     "iopub.status.busy": "2025-01-22T19:20:07.820834Z",
     "iopub.status.idle": "2025-01-22T19:20:07.845999Z",
     "shell.execute_reply": "2025-01-22T19:20:07.845081Z"
    },
    "papermill": {
     "duration": 0.03046,
     "end_time": "2025-01-22T19:20:07.847809",
     "exception": false,
     "start_time": "2025-01-22T19:20:07.817349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parquet_dirs = [\n",
    "    os.path.join(PARQUET_DIR, f) for f in os.listdir(PARQUET_DIR)\n",
    "]\n",
    "\n",
    "valid_dataloader = create_dataloader([parquet_dirs[0]], config[\"batch_size\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a051f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T19:20:07.855461Z",
     "iopub.status.busy": "2025-01-22T19:20:07.854583Z",
     "iopub.status.idle": "2025-01-22T19:20:09.897323Z",
     "shell.execute_reply": "2025-01-22T19:20:09.896286Z"
    },
    "papermill": {
     "duration": 2.048223,
     "end_time": "2025-01-22T19:20:09.898841",
     "exception": false,
     "start_time": "2025-01-22T19:20:07.850618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all model params from /home/A.Y/project/MethylGPT_clean/pretrained_models/dev_pretraining_test-dataset_CpGs_type3-preprocessing_False-Sep26-10-27/model_epoch10.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.embedding.weight torch.float16\n",
      "encoder.enc_norm.weight torch.float16\n",
      "encoder.enc_norm.bias torch.float16\n",
      "value_encoder.linear1.weight torch.float16\n",
      "value_encoder.linear1.bias torch.float16\n",
      "value_encoder.linear2.weight torch.float16\n",
      "value_encoder.linear2.bias torch.float16\n",
      "value_encoder.norm.weight torch.float16\n",
      "value_encoder.norm.bias torch.float16\n",
      "transformer_encoder.layers.0.self_attn.Wqkv.weight torch.float16\n",
      "transformer_encoder.layers.0.self_attn.Wqkv.bias torch.float16\n",
      "transformer_encoder.layers.0.self_attn.out_proj.weight torch.float16\n",
      "transformer_encoder.layers.0.self_attn.out_proj.bias torch.float16\n",
      "transformer_encoder.layers.0.linear1.weight torch.float16\n",
      "transformer_encoder.layers.0.linear1.bias torch.float16\n",
      "transformer_encoder.layers.0.linear2.weight torch.float16\n",
      "transformer_encoder.layers.0.linear2.bias torch.float16\n",
      "transformer_encoder.layers.0.norm1.weight torch.float16\n",
      "transformer_encoder.layers.0.norm1.bias torch.float16\n",
      "transformer_encoder.layers.0.norm2.weight torch.float16\n",
      "transformer_encoder.layers.0.norm2.bias torch.float16\n",
      "transformer_encoder.layers.1.self_attn.Wqkv.weight torch.float16\n",
      "transformer_encoder.layers.1.self_attn.Wqkv.bias torch.float16\n",
      "transformer_encoder.layers.1.self_attn.out_proj.weight torch.float16\n",
      "transformer_encoder.layers.1.self_attn.out_proj.bias torch.float16\n",
      "transformer_encoder.layers.1.linear1.weight torch.float16\n",
      "transformer_encoder.layers.1.linear1.bias torch.float16\n",
      "transformer_encoder.layers.1.linear2.weight torch.float16\n",
      "transformer_encoder.layers.1.linear2.bias torch.float16\n",
      "transformer_encoder.layers.1.norm1.weight torch.float16\n",
      "transformer_encoder.layers.1.norm1.bias torch.float16\n",
      "transformer_encoder.layers.1.norm2.weight torch.float16\n",
      "transformer_encoder.layers.1.norm2.bias torch.float16\n",
      "transformer_encoder.layers.2.self_attn.Wqkv.weight torch.float16\n",
      "transformer_encoder.layers.2.self_attn.Wqkv.bias torch.float16\n",
      "transformer_encoder.layers.2.self_attn.out_proj.weight torch.float16\n",
      "transformer_encoder.layers.2.self_attn.out_proj.bias torch.float16\n",
      "transformer_encoder.layers.2.linear1.weight torch.float16\n",
      "transformer_encoder.layers.2.linear1.bias torch.float16\n",
      "transformer_encoder.layers.2.linear2.weight torch.float16\n",
      "transformer_encoder.layers.2.linear2.bias torch.float16\n",
      "transformer_encoder.layers.2.norm1.weight torch.float16\n",
      "transformer_encoder.layers.2.norm1.bias torch.float16\n",
      "transformer_encoder.layers.2.norm2.weight torch.float16\n",
      "transformer_encoder.layers.2.norm2.bias torch.float16\n",
      "transformer_encoder.layers.3.self_attn.Wqkv.weight torch.float16\n",
      "transformer_encoder.layers.3.self_attn.Wqkv.bias torch.float16\n",
      "transformer_encoder.layers.3.self_attn.out_proj.weight torch.float16\n",
      "transformer_encoder.layers.3.self_attn.out_proj.bias torch.float16\n",
      "transformer_encoder.layers.3.linear1.weight torch.float16\n",
      "transformer_encoder.layers.3.linear1.bias torch.float16\n",
      "transformer_encoder.layers.3.linear2.weight torch.float16\n",
      "transformer_encoder.layers.3.linear2.bias torch.float16\n",
      "transformer_encoder.layers.3.norm1.weight torch.float16\n",
      "transformer_encoder.layers.3.norm1.bias torch.float16\n",
      "transformer_encoder.layers.3.norm2.weight torch.float16\n",
      "transformer_encoder.layers.3.norm2.bias torch.float16\n",
      "transformer_encoder.layers.4.self_attn.Wqkv.weight torch.float16\n",
      "transformer_encoder.layers.4.self_attn.Wqkv.bias torch.float16\n",
      "transformer_encoder.layers.4.self_attn.out_proj.weight torch.float16\n",
      "transformer_encoder.layers.4.self_attn.out_proj.bias torch.float16\n",
      "transformer_encoder.layers.4.linear1.weight torch.float16\n",
      "transformer_encoder.layers.4.linear1.bias torch.float16\n",
      "transformer_encoder.layers.4.linear2.weight torch.float16\n",
      "transformer_encoder.layers.4.linear2.bias torch.float16\n",
      "transformer_encoder.layers.4.norm1.weight torch.float16\n",
      "transformer_encoder.layers.4.norm1.bias torch.float16\n",
      "transformer_encoder.layers.4.norm2.weight torch.float16\n",
      "transformer_encoder.layers.4.norm2.bias torch.float16\n",
      "transformer_encoder.layers.5.self_attn.Wqkv.weight torch.float16\n",
      "transformer_encoder.layers.5.self_attn.Wqkv.bias torch.float16\n",
      "transformer_encoder.layers.5.self_attn.out_proj.weight torch.float16\n",
      "transformer_encoder.layers.5.self_attn.out_proj.bias torch.float16\n",
      "transformer_encoder.layers.5.linear1.weight torch.float16\n",
      "transformer_encoder.layers.5.linear1.bias torch.float16\n",
      "transformer_encoder.layers.5.linear2.weight torch.float16\n",
      "transformer_encoder.layers.5.linear2.bias torch.float16\n",
      "transformer_encoder.layers.5.norm1.weight torch.float16\n",
      "transformer_encoder.layers.5.norm1.bias torch.float16\n",
      "transformer_encoder.layers.5.norm2.weight torch.float16\n",
      "transformer_encoder.layers.5.norm2.bias torch.float16\n",
      "decoder.fc.0.weight torch.float16\n",
      "decoder.fc.0.bias torch.float16\n",
      "decoder.fc.2.weight torch.float16\n",
      "decoder.fc.2.bias torch.float16\n",
      "decoder.fc.4.weight torch.float16\n",
      "decoder.fc.4.bias torch.float16\n",
      "cls_decoder._decoder.0.weight torch.float16\n",
      "cls_decoder._decoder.0.bias torch.float16\n",
      "cls_decoder._decoder.2.weight torch.float16\n",
      "cls_decoder._decoder.2.bias torch.float16\n",
      "cls_decoder._decoder.3.weight torch.float16\n",
      "cls_decoder._decoder.3.bias torch.float16\n",
      "cls_decoder._decoder.5.weight torch.float16\n",
      "cls_decoder._decoder.5.bias torch.float16\n",
      "cls_decoder.out_layer.weight torch.float16\n",
      "cls_decoder.out_layer.bias torch.float16\n",
      "mvc_decoder.gene2query.weight torch.float16\n",
      "mvc_decoder.gene2query.bias torch.float16\n",
      "mvc_decoder.W.weight torch.float16\n"
     ]
    }
   ],
   "source": [
    "methyl_vocab = MethylVocab(config[\"probe_id_dir\"], config[\"pad_token\"], config[\"special_tokens\"], save_dir=None)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MethylGPTModel(config, methyl_vocab)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(MODEL_DIR, map_location=\"cpu\"))\n",
    "    print(f\"Loading all model params from {MODEL_DIR}\")\n",
    "except:\n",
    "    # only load params that are in the model and match the size\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(MODEL_DIR, map_location=\"cpu\")\n",
    "    pretrained_dict = {\n",
    "        k: v\n",
    "        for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and v.shape == model_dict[k].shape\n",
    "    }\n",
    "    for k, v in pretrained_dict.items():\n",
    "        print(f\"Loading params {k} with shape {v.shape}\")\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "model.eval()  # Switch to evaluation mode (turns off dropout, etc.)\n",
    "model.to(device)\n",
    "model.half()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92fb8a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T19:20:09.906861Z",
     "iopub.status.busy": "2025-01-22T19:20:09.906148Z",
     "iopub.status.idle": "2025-01-22T19:30:33.098885Z",
     "shell.execute_reply": "2025-01-22T19:30:33.097769Z"
    },
    "papermill": {
     "duration": 623.198921,
     "end_time": "2025-01-22T19:30:33.101029",
     "exception": false,
     "start_time": "2025-01-22T19:20:09.902108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Too many dataloader workers: 24 (max is dataset.n_shards=1). Stopping 23 dataloader workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 1it [00:14, 14.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 2it [00:20,  9.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 3it [00:26,  7.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 4it [00:32,  7.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 5it [00:38,  6.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 6it [00:43,  6.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 7it [00:49,  6.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 8it [00:55,  6.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 9it [01:02,  6.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 10it [01:08,  6.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 11it [01:14,  6.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 12it [01:20,  6.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 13it [01:26,  6.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 14it [01:32,  6.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 15it [01:38,  6.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 16it [01:44,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 17it [01:51,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 18it [01:57,  6.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 19it [02:03,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 20it [02:09,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 21it [02:15,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 22it [02:21,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 23it [02:27,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 24it [02:33,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 25it [02:40,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 26it [02:46,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 27it [02:52,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 28it [02:58,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 29it [03:04,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 30it [03:10,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 31it [03:16,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 32it [03:22,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 33it [03:29,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 34it [03:35,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 35it [03:41,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 36it [03:47,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 37it [03:53,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 38it [03:59,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 39it [04:05,  6.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 40it [04:11,  5.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 41it [04:17,  5.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 42it [04:23,  6.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 43it [04:29,  6.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 44it [04:35,  6.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 45it [04:41,  6.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 46it [04:48,  6.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 47it [04:54,  6.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 48it [05:00,  6.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 49it [05:06,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 50it [05:12,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 51it [05:18,  6.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 52it [05:24,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 53it [05:30,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 54it [05:37,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 55it [05:43,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 56it [05:49,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 57it [05:55,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 58it [06:01,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 59it [06:07,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 60it [06:13,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 61it [06:19,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 62it [06:26,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 63it [06:32,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 64it [06:38,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 65it [06:44,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 66it [06:50,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 67it [06:56,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 68it [07:02,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 69it [07:08,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 70it [07:15,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 71it [07:21,  6.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 72it [07:27,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 73it [07:33,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 74it [07:39,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 75it [07:45,  6.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 76it [07:51,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 77it [07:57,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 78it [08:04,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 79it [08:10,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 80it [08:16,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 81it [08:22,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 82it [08:28,  6.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 83it [08:34,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 84it [08:40,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 85it [08:47,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 86it [08:53,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 87it [08:59,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 88it [09:05,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 89it [09:11,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 90it [09:17,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 91it [09:23,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 92it [09:29,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 93it [09:35,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 94it [09:42,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 95it [09:48,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 96it [09:54,  6.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 97it [10:00,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 98it [10:06,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 99it [10:12,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 100it [10:18,  6.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing batches: 100it [10:23,  6.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_cell_embeddings(model, data_loader, device, vocab, max_seq_len, config, mask_value, pad_value, pad_token):\n",
    "    \"\"\"\n",
    "    Generate cell embeddings using the provided model and data loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to generate embeddings.\n",
    "        data_loader (torch.utils.data.DataLoader): DataLoader containing the dataset.\n",
    "        device (torch.device): The device to run the model on.\n",
    "        vocab (dict): Vocabulary dictionary.\n",
    "        max_seq_len (int): Maximum sequence length.\n",
    "        config (object): Configuration object containing mask_ratio.\n",
    "        mask_value (float): Value used for masking.\n",
    "        pad_value (float): Value used for padding.\n",
    "        pad_token (str): Token used for padding.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of cell embeddings.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If there's an error during embedding generation.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    cell_embs = []\n",
    "    cell_ids = []\n",
    "    \n",
    "    logger.info(\"Generating embedding...\")\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(data_loader, desc=\"Processing batches\")):\n",
    "            # Prepare data\n",
    "\n",
    "            if i==100:\n",
    "                break\n",
    "            batch_data = model.prepare_data(batch)\n",
    "            \n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device).half()\n",
    "            \n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token]).to(device)\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                MVC=config[\"GEPC\"],\n",
    "                ECS=config[\"ecs_thres\"] > 0,\n",
    "            )\n",
    "            output_values = output_dict[\"cell_emb\"].cpu().numpy()\n",
    "            cell_embs.append(output_values)\n",
    "            cell_ids.append(batch[\"id\"])\n",
    "            \n",
    "            logger.debug(f\"Batch embedding shape: {output_values.shape}\")\n",
    "\n",
    "            \n",
    "    \n",
    "    cell_emb = np.concatenate(cell_embs, axis=0)\n",
    "    cell_list = np.concatenate(cell_ids, axis=0)\n",
    "    logger.info(f\"Validset embedding shape: {cell_emb.shape}\")\n",
    "    return cell_emb, cell_list\n",
    "\n",
    "    #except Exception as e:\n",
    "    #    logger.error(f\"Error generating cell embeddings: {str(e)}\")\n",
    "    #    raise RuntimeError(\"Failed to generate cell embeddings\") from e\n",
    "\n",
    "\n",
    "valid_cell_emb, valid_cell_list = generate_cell_embeddings(\n",
    "    model, \n",
    "    valid_dataloader, \n",
    "    device,\n",
    "    methyl_vocab,\n",
    "    max_seq_len,\n",
    "    config,\n",
    "    mask_value,\n",
    "    pad_value,\n",
    "    pad_token\n",
    ")\n",
    "valid_emb_path = SAVE_DIR / \"cell_emb.pt\"\n",
    "with open(valid_emb_path, \"wb\") as file:\n",
    "    pickle.dump({\"cell_emb\": valid_cell_emb, \"cell_list\": valid_cell_list}, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22997f79",
   "metadata": {
    "papermill": {
     "duration": 0.015675,
     "end_time": "2025-01-22T19:30:33.135202",
     "exception": false,
     "start_time": "2025-01-22T19:30:33.119527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff7883",
   "metadata": {
    "papermill": {
     "duration": 0.015575,
     "end_time": "2025-01-22T19:30:33.166784",
     "exception": false,
     "start_time": "2025-01-22T19:30:33.151209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 640.546033,
   "end_time": "2025-01-22T19:30:35.307180",
   "environment_variables": {},
   "exception": null,
   "input_path": "get_embeddings.ipynb",
   "output_path": "get_embeddings.ipynb",
   "parameters": {},
   "start_time": "2025-01-22T19:19:54.761147",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
